{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deven\\AppData\\Local\\Temp\\ipykernel_37096\\2810747793.py:1: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(\"D:\\ECH\\Dataset\\RecomposedESNI4.csv\", header=None, names=['target', *(f'C{i:02}' for i in range(284))])\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"D:\\ECH\\Dataset\\RecomposedESNI4.csv\", header=None, names=['target', *(f'C{i:02}' for i in range(284))])\n",
    "df=df.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>C00</th>\n",
       "      <th>C01</th>\n",
       "      <th>C02</th>\n",
       "      <th>C03</th>\n",
       "      <th>C04</th>\n",
       "      <th>C05</th>\n",
       "      <th>C06</th>\n",
       "      <th>C07</th>\n",
       "      <th>C08</th>\n",
       "      <th>...</th>\n",
       "      <th>C274</th>\n",
       "      <th>C275</th>\n",
       "      <th>C276</th>\n",
       "      <th>C277</th>\n",
       "      <th>C278</th>\n",
       "      <th>C279</th>\n",
       "      <th>C280</th>\n",
       "      <th>C281</th>\n",
       "      <th>C282</th>\n",
       "      <th>C283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AppleMusic</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AppleMusic</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AppleMusic</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AppleMusic</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AppleMusic</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>YouTube_PC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>YouTube_PC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>YouTube_PC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>YouTube_PC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>YouTube_PC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3602 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target C00 C01  C02    C03  C04  C05    C06  C07  C08  ...  C274  \\\n",
       "1     AppleMusic   3   1  2.0    0.0  0.0  1.0  252.0  3.0  3.0  ...   0.0   \n",
       "2     AppleMusic   3   1  2.0   71.0  0.0  2.0   67.0  3.0  3.0  ...   0.0   \n",
       "3     AppleMusic   3   1  2.0    0.0  0.0  1.0  252.0  3.0  3.0  ...   0.0   \n",
       "4     AppleMusic   3   1  2.0   71.0  0.0  2.0   67.0  3.0  3.0  ...   0.0   \n",
       "5     AppleMusic   3   1  2.0   71.0  0.0  2.0   67.0  3.0  3.0  ...   0.0   \n",
       "...          ...  ..  ..  ...    ...  ...  ...    ...  ...  ...  ...   ...   \n",
       "3598  YouTube_PC   3   1  0.0  215.0  0.0  0.0  211.0  3.0  3.0  ...   0.0   \n",
       "3599  YouTube_PC   3   1  0.0  215.0  0.0  0.0  211.0  3.0  3.0  ...   0.0   \n",
       "3600  YouTube_PC   3   1  0.0  215.0  0.0  0.0  211.0  3.0  3.0  ...   0.0   \n",
       "3601  YouTube_PC   3   1  2.0    0.0  0.0  1.0  252.0  3.0  3.0  ...   0.0   \n",
       "3602  YouTube_PC   3   1  2.0    0.0  0.0  1.0  252.0  3.0  3.0  ...   0.0   \n",
       "\n",
       "      C275  C276  C277  C278  C279  C280  C281  C282  C283  \n",
       "1      0.0   0.0   0.0   0.0  36.0   0.0  29.0   3.0   4.0  \n",
       "2      0.0   0.0   2.0   0.0  36.0   0.0  29.0   3.0   4.0  \n",
       "3      0.0   0.0   0.0   0.0  36.0   0.0  29.0   3.0   4.0  \n",
       "4      0.0   0.0   2.0   0.0  36.0   0.0  29.0   3.0   4.0  \n",
       "5      0.0   0.0   2.0   0.0  36.0   0.0  29.0   3.0   4.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "3598   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3599   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3600   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3601   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3602   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3602 rows x 285 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.replace(r'\\t', '', regex=True, inplace=True)\n",
    "target = df.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       AppleMusic\n",
       "2       AppleMusic\n",
       "3       AppleMusic\n",
       "4       AppleMusic\n",
       "5       AppleMusic\n",
       "           ...    \n",
       "3598    YouTube_PC\n",
       "3599    YouTube_PC\n",
       "3600    YouTube_PC\n",
       "3601    YouTube_PC\n",
       "3602    YouTube_PC\n",
       "Name: target, Length: 3602, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "ww               1045\n",
      "Netflix           433\n",
      "YandexMusic       376\n",
      "AppleMusic        292\n",
      "SoundCloud        281\n",
      "Kinopoisk         268\n",
      "Spotify           255\n",
      "YouTube_PC        249\n",
      "PrimeVideo        191\n",
      "Live_Facebook     106\n",
      "Vimeo             106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "value_counts = target.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       AppleMusic\n",
       "2       AppleMusic\n",
       "3       AppleMusic\n",
       "4       AppleMusic\n",
       "5       AppleMusic\n",
       "           ...    \n",
       "3598    YouTube_PC\n",
       "3599    YouTube_PC\n",
       "3600    YouTube_PC\n",
       "3601    YouTube_PC\n",
       "3602    YouTube_PC\n",
       "Name: target, Length: 3602, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the target values\n",
    "y= label_encoder.fit_transform(target)\n",
    "\n",
    "# Convert the encoded labels to a DataFrame or Series if needed\n",
    "target = pd.Series(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample DataFrames (replace with your actual data)\n",
    "# df_modified: DataFrame of features with shape (3547, 18)\n",
    "# target: Series or DataFrame of target values\n",
    "X = df\n",
    "y = target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "num_replicates = 200\n",
    "\n",
    "# Initializing lists to store replicated test sets\n",
    "X_test_replicates = []\n",
    "y_test_replicates = []\n",
    "\n",
    "for _ in range(num_replicates):\n",
    "    X_test_replicates.append(X_test.copy())\n",
    "    y_test_replicates.append(y_test.copy())\n",
    "\n",
    "X_test_replicates = np.array(X_test_replicates)\n",
    "y_test_replicates = np.array(y_test_replicates)\n",
    "\n",
    "# Save the datasets\n",
    "# You can save them to a file or keep them in the variables\n",
    "# For example, if you want to keep them in variables for further processing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 1.1261 seconds\n",
      "\n",
      "Class-wise Performance Table:\n",
      "            Class  Accuracy (%)  Error Rate (%)  Precision (%)  Recall (%)\n",
      "0      AppleMusic    100.000000        0.000000     100.000000  100.000000\n",
      "1       Kinopoisk     98.148148        1.851852     100.000000   98.148148\n",
      "2   Live_Facebook    100.000000        0.000000     100.000000  100.000000\n",
      "3         Netflix    100.000000        0.000000     100.000000  100.000000\n",
      "4      PrimeVideo    100.000000        0.000000     100.000000  100.000000\n",
      "5      SoundCloud     98.214286        1.785714     100.000000   98.214286\n",
      "6         Spotify    100.000000        0.000000      98.076923  100.000000\n",
      "7           Vimeo    100.000000        0.000000     100.000000  100.000000\n",
      "8     YandexMusic     98.666667        1.333333      98.666667   98.666667\n",
      "9      YouTube_PC     98.000000        2.000000     100.000000   98.000000\n",
      "10             ww     99.521531        0.478469      98.578199   99.521531\n",
      "\n",
      "Overall Accuracy: 0.9931\n",
      "F1 Score: 0.9931\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 59   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  53   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0  21   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  87   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  38   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  55   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0  51   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  21   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  74   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0  49   1]\n",
      " [  0   0   0   0   0   0   0   0   1   0 208]]\n",
      "Prediction Time: 0.0675 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"Evaluate the model and print the performance metrics.\"\"\"\n",
    "    # Predict labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert labels back to original class labels\n",
    "    y_test_classes = label_encoder.inverse_transform(y_test)\n",
    "    y_pred_classes = label_encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test_classes, y_pred_classes, labels=label_encoder.classes_)\n",
    "    \n",
    "    # Calculate error rates per class\n",
    "    class_totals = np.sum(conf_matrix, axis=1)\n",
    "    class_errors = class_totals - np.diag(conf_matrix)\n",
    "    class_error_rates = class_errors / class_totals\n",
    "    class_error_rates_percentage = class_error_rates * 100\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average=None)\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average=None)\n",
    "    \n",
    "    # Create a DataFrame for class-wise performance\n",
    "    accuracy_df = pd.DataFrame({\n",
    "        'Class': label_encoder.classes_,\n",
    "        'Accuracy (%)': 100 - class_error_rates_percentage,\n",
    "        'Error Rate (%)': class_error_rates_percentage,\n",
    "        'Precision (%)': precision * 100,\n",
    "        'Recall (%)': recall * 100\n",
    "    })\n",
    "    \n",
    "    print(\"\\nClass-wise Performance Table:\")\n",
    "    print(accuracy_df)\n",
    "    \n",
    "    # Print overall accuracy\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Compute and print F1 score\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "def main():\n",
    "    # File path to your CSV file\n",
    "    # data = pd.read_csv('your_file.csv')  # Uncomment and set your CSV file path\n",
    "    \n",
    "    # Assuming 'data' is your DataFrame\n",
    "    # label_encoder = LabelEncoder()\n",
    "    # data['label'] = label_encoder.fit_transform(data['label'])  # Adjust this line based on your label column\n",
    "    \n",
    "    # Split data into features and target\n",
    "    # X = data.drop('label', axis=1)  # Adjust based on your DataFrame\n",
    "    # y = data['label']  # Adjust based on your DataFrame\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Random Forest Classifier with min_samples_split set to 10\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=150, \n",
    "        min_samples_split=10,\n",
    "        max_features=35,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(rf_classifier, X_test, y_test, label_encoder)\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    print(f\"Prediction Time: {prediction_time:.4f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model after SMOTE...\n",
      "\n",
      "Cross-Validation Scores: [0.99619565 0.99673736 0.99836868 0.99836868 0.99945623]\n",
      "Mean Cross-Validation Score: 0.9978\n",
      "\n",
      "Confusion Matrix:\n",
      "[[214   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 217   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 211   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 188   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 199   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0 233   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 200   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 235   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 212   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 182   0]\n",
      " [  0   0   0   0   0   1   0   0   1   0 205]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your data\n",
    "# df = pd.read_csv('your_data.csv')  # Uncomment and set your CSV file path\n",
    "# Assuming df is your features DataFrame and target is your target Series\n",
    "# target = df['target_column_name']\n",
    "\n",
    "# Sample data (for illustration)\n",
    "# Replace this with your actual DataFrame\n",
    "X = df  # Features DataFrame\n",
    "y = target  # Target Series\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Optional: Using SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate the model using cross-validation and print the performance metrics.\"\"\"\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    \n",
    "    print(f\"\\nCross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean Cross-Validation Score: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "    # Train the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "\n",
    "    # Print confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "def main():\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Random Forest Classifier with class weights\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_features=35,  # Adjust this as needed\n",
    "        random_state=42,\n",
    "        class_weight=class_weights_dict  # Use class weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating model after SMOTE...\")\n",
    "    evaluate_model(rf_classifier, X_train, y_train, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model after SMOTE...\n",
      "\n",
      "Cross-Validation Scores: [0.99409237 0.99892531 1.         0.99785062 0.99785062]\n",
      "Mean Cross-Validation Score: 0.9977\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0  68   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  22   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  88   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  36   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  59   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  42   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  19   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  67   0   2]\n",
      " [  0   0   0   0   0   0   0   0   0  57   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 199]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your data\n",
    "# df = pd.read_csv('your_data.csv')  # Uncomment and set your CSV file path\n",
    "# Assuming df is your features DataFrame and target is your target Series\n",
    "# target = df['target_column_name']\n",
    "\n",
    "# Sample data (for illustration)\n",
    "# Replace this with your actual DataFrame\n",
    "X = df  # Features DataFrame\n",
    "y = target  # Target Series\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # Optional: Using SMOTE to balance the dataset\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate the model using cross-validation and print the performance metrics.\"\"\"\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    \n",
    "    print(f\"\\nCross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean Cross-Validation Score: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "    # Train the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "\n",
    "    # Print confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "def main():\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Optional: Using SMOTE to balance the dataset\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Initialize Random Forest Classifier with class weights\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_features=35,  # Adjust this as needed\n",
    "        random_state=42,\n",
    "        class_weight=class_weights_dict  # Use class weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating model after SMOTE...\")\n",
    "    evaluate_model(rf_classifier, X_resampled, y_resampled, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn->imblearn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\deven\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn->imblearn) (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "Best Parameters: {'class_weight': None, 'max_features': 35, 'min_samples_split': 10, 'n_estimators': 400}\n",
      "\n",
      "Class-wise Performance Table:\n",
      "            Class  Accuracy (%)  Error Rate (%)\n",
      "0      AppleMusic    100.000000        0.000000\n",
      "1       Kinopoisk     98.148148        1.851852\n",
      "2   Live_Facebook    100.000000        0.000000\n",
      "3         Netflix    100.000000        0.000000\n",
      "4      PrimeVideo    100.000000        0.000000\n",
      "5      SoundCloud     98.214286        1.785714\n",
      "6         Spotify    100.000000        0.000000\n",
      "7           Vimeo    100.000000        0.000000\n",
      "8     YandexMusic     98.666667        1.333333\n",
      "9      YouTube_PC     98.000000        2.000000\n",
      "10             ww     99.521531        0.478469\n",
      "\n",
      "Overall Accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"Evaluate the model and print the performance metrics.\"\"\"\n",
    "    # Predict labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert labels back to original class labels\n",
    "    y_test_classes = label_encoder.inverse_transform(y_test)\n",
    "    y_pred_classes = label_encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test_classes, y_pred_classes, labels=label_encoder.classes_)\n",
    "    \n",
    "    # Calculate error rates per class\n",
    "    class_totals = np.sum(conf_matrix, axis=1)\n",
    "    class_errors = class_totals - np.diag(conf_matrix)\n",
    "    class_error_rates = class_errors / class_totals\n",
    "    class_error_rates_percentage = class_error_rates * 100\n",
    "    \n",
    "    # Create a DataFrame for class-wise performance\n",
    "    accuracy_df = pd.DataFrame({\n",
    "        'Class': label_encoder.classes_,\n",
    "        'Accuracy (%)': 100 - class_error_rates_percentage,\n",
    "        'Error Rate (%)': class_error_rates_percentage\n",
    "    })\n",
    "    \n",
    "    print(\"\\nClass-wise Performance Table:\")\n",
    "    print(accuracy_df)\n",
    "    \n",
    "    # Print overall accuracy\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Load your data (replace with your actual file path)\n",
    "    # df = pd.read_csv('your_data.csv')\n",
    "    # target = df['target_column_name']\n",
    "    \n",
    "    X = df  # Replace with your features DataFrame\n",
    "    \n",
    "    y_encoded=target\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [250,300,400,150,100],\n",
    "        'max_features': [70,35],\n",
    "        'min_samples_split': [10],\n",
    "        'class_weight': [None,'balanced']\n",
    "    }\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Fit the model using GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters and best estimator\n",
    "    print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "    best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    evaluate_model(best_rf_classifier, X_test, y_test, label_encoder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
